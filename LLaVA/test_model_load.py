import torch
from transformers import AutoTokenizer, CLIPVisionModel
from llava.model import LlavaLlamaForCausalLM

model_path = "./checkpoints/llava-v1.5-7b"
vision_path = "./checkpoints/clip-vit-large-patch14-336"

print("ğŸ”„ åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("ğŸ”„ åŠ è½½ LLaVA æ¨¡å‹...")
model = LlavaLlamaForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("ğŸ”„ åŠ è½½è§†è§‰å¡”...")
vision_model = CLIPVisionModel.from_pretrained(
    vision_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("âœ… æ¨¡å‹å’Œè§†è§‰å¡”åŠ è½½æˆåŠŸï¼")
